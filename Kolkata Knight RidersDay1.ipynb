{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pymongo\n",
    "import ipywidgets as wgt\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-b2138e576d57>:11: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  col.count()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = \"pCRLPdf2lkjpRX9Uczpt2gVBb\" \n",
    "api_secret = \"G1Wv7sTLMYF3cr5LbL8S9uj2g2LFrdWp8X8t7Ft3sIpW0kbqgD\" \n",
    "access_token = \"100875722-7m5vhOzpw2LYrB7P8ISnTqtGYMMvVTqwx3X4EyJY\" \n",
    "access_token_secret = \"uPEymGiszhiJEVWD4Alo1pzMUJx1ymo3Fd4HWyRPRBN3r\" \n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "col = pymongo.MongoClient()[\"DataWranglingDay1\"][\"Kolkata Knight Riders\"]\n",
    "col.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a028c215e49486e90ebbf836e474e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba16d305de7e4762ba39c072c43ccb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<span class=\"label label-primary\">Tweets/Sec: 0.0</span>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    def __init__(self, max_tweets=3000, *args, **kwargs):\n",
    "        self.max_tweets = max_tweets\n",
    "        self.counter = 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def on_connect(self):\n",
    "        self.counter = 0\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "        \n",
    "        col.insert_one(status._json)\n",
    "        \n",
    "        \n",
    "        if self.counter % 1 == 0:\n",
    "            value = int(100.00 * self.counter / self.max_tweets)\n",
    "            mining_time = datetime.now() - self.start_time\n",
    "            progress_bar.value = value\n",
    "            html_value = \"\"\"<span class=\"label label-primary\">Tweets/Sec: %.1f</span>\"\"\" % (self.counter / max([1,mining_time.seconds]))\n",
    "            html_value += \"\"\" <span class=\"label label-success\">Progress: %.1f%%</span>\"\"\" % (self.counter / self.max_tweets * 100.0)\n",
    "            html_value += \"\"\" <span class=\"label label-info\">ETA: %.1f Sec</span>\"\"\" % ((self.max_tweets - self.counter) / (self.counter / max([1,mining_time.seconds])))\n",
    "            wgt_status.value = html_value\n",
    "           \n",
    "            if self.counter >= self.max_tweets:\n",
    "                myStream.disconnect()\n",
    "                print(\"Finished\")\n",
    "                print(\"Total Mining Time: %s\" % (mining_time))\n",
    "                print(\"Tweets/Sec: %.1f\" % (self.max_tweets / mining_time.seconds))\n",
    "                progress_bar.value = 0\n",
    "                \n",
    "    \n",
    "myStreamListener = MyStreamListener(max_tweets=3000)\n",
    "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)\n",
    "keywords = [\"Kolkata Knight Riders\",\"\"\n",
    "           ]\n",
    "\n",
    "\n",
    "progress_bar = wgt.IntProgress(value=0)\n",
    "display(progress_bar)\n",
    "wgt_status = wgt.HTML(value=\"\"\"<span class=\"label label-primary\">Tweets/Sec: 0.0</span>\"\"\")\n",
    "display(wgt_status)\n",
    "\n",
    "\n",
    "for error_counter in range(20):\n",
    "    try:\n",
    "        myStream.filter(track=keywords)\n",
    "        print(\"Tweets collected: %s\" % myStream.listener.counter)\n",
    "        print(\"Total tweets in collection: %s\" % col.count())\n",
    "        break\n",
    "    except:\n",
    "        print(\"ERROR# %s\" % (error_counter + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [{\"created_at\": item[\"created_at\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"user\": \"@%s\" % item[\"user\"][\"screen_name\"],\n",
    "            \"source\": item[\"source\"],\n",
    "            \"lang\":item[\"lang\"],\n",
    "            \n",
    "           } for item in col.find()]\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-17e71fac3833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcount_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(dataset.text)\n",
    "\n",
    "word_count = pd.DataFrame(cv.get_feature_names(), columns=[\"word\"])\n",
    "word_count[\"count\"] = count_matrix.sum(axis=0).tolist()[0]\n",
    "word_count = word_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "word_count[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a8481ec67a37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_source_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msource_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'source'"
     ]
    }
   ],
   "source": [
    "def get_source_name(x):\n",
    "    value = re.findall(pattern=\"<[^>]+>([^<]+)</a>\", string=x)\n",
    "    if len(value) > 0:\n",
    "        return value[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "dataset.source_name = dataset.source.apply(get_source_name)\n",
    "\n",
    "source_counts = dataset.source_name.value_counts().sort_values()[-10:]\n",
    "\n",
    "bottom = [index for index, item in enumerate(source_counts.index)]\n",
    "plt.barh(bottom, width=source_counts, color=\"blue\", linewidth=0)\n",
    "\n",
    "y_labels = [\"%s %.1f%%\" % (item, 100.0*source_counts[item]/len(dataset)) for index,item in enumerate(source_counts.index)]\n",
    "plt.yticks(np.array(bottom)+0.4, y_labels)\n",
    "\n",
    "source_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_counts = dataset.lang.value_counts().sort_values()[-10:]\n",
    "bottom = [index for index, item in enumerate(lang_counts.index)]\n",
    "plt.barh(bottom, width=lang_counts, color=\"green\", linewidth=0)\n",
    "y_labels = [\"%s %.1f%%\" % (item, 100.0*lang_counts[item]/len(dataset)) for index,item in enumerate(lang_counts.index)]\n",
    "plt.yticks(np.array(bottom)+0.4, y_labels)\n",
    "lang_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from  textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = [{\"id\": item[\"id\"],\n",
    "            \"text\": item[\"text\"],\n",
    "            \"lang\":item[\"lang\"],\n",
    "            \n",
    "           } for item in col.find()]\n",
    "\n",
    "dataset1 = pd.DataFrame(dataset1)\n",
    "\n",
    "is_dataset1=dataset1.lang==\"en\"\n",
    "dataset1=dataset1[is_dataset1]\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweettext=dataset1['text']\n",
    "wordlist=pd.DataFrame();\n",
    "\n",
    "polarity=[]\n",
    "subj=[]\n",
    "\n",
    "for t in tweettext:\n",
    "    tx= TextBlob(t)\n",
    "    polarity.append(tx.sentiment.polarity)\n",
    "    subj.append(tx.sentiment.subjectivity)\n",
    "\n",
    "poltweet= pd.DataFrame({'polarity':polarity,'subjectivity':subj})\n",
    "poltweet.polarity.plot(title='Polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poltweet.subjectivity.plot(title='Subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "statistics.mean(poltweet.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "statistics.mean(poltweet.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def wc(data,bgcolor,title):\n",
    "    plt.figure(figsize = (100,100))\n",
    "    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n",
    "    wc.generate(' '.join(data))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "top_N = 100\n",
    "\n",
    "\n",
    "a = dataset1['text'].str.lower().str.cat(sep=' ')\n",
    "\n",
    "\n",
    "b = re.sub('[^A-Za-z]+', ' ', a)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = list(get_stop_words('en'))         \n",
    "nltk_words = list(stopwords.words('english'))   \n",
    "stop_words.extend(nltk_words)\n",
    "word_tokens = word_tokenize(b)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    " \n",
    "without_single_chr = [word for word in filtered_sentence if len(word) > 2]\n",
    "\n",
    "\n",
    "cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]        \n",
    "\n",
    "\n",
    "word_dist = nltk.FreqDist(cleaned_data_title)\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Word\",y=\"Frequency\", data=rslt.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc(cleaned_data_title,'black','Common Words' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "bloblist_desc = list()\n",
    "\n",
    "twit=dataset1['text'].astype(str)\n",
    "for row in twit:\n",
    "    blob = TextBlob(row)\n",
    "    bloblist_desc.append((row,blob.sentiment.polarity, blob.sentiment.subjectivity))\n",
    "    twit_polarity_desc = pd.DataFrame(bloblist_desc, columns = ['sentence','sentiment','polarity'])\n",
    "    \n",
    "def f(twit_polarity_desc):\n",
    "    if twit_polarity_desc['sentiment'] > 0:\n",
    "        val = \"Positive\"\n",
    "    elif twit_polarity_desc['sentiment'] == 0:\n",
    "        val = \"Neutral\"\n",
    "    else:\n",
    "        val = \"Negative\"\n",
    "    return val\n",
    "\n",
    "twit_polarity_desc['Sentiment_Type'] = twit_polarity_desc.apply(f, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections, numpy as np\n",
    "a=twit_polarity_desc.Sentiment_Type\n",
    "neut=np.count_nonzero(a == 'Neutral')\n",
    "pos=np.count_nonzero(a == 'Positive')\n",
    "neg=np.count_nonzero(a == 'Negative')\n",
    "popularity='{0:.2f}'.format((((neut*0.5)+(pos)+(neg*0))/(neut+pos+neg))*100)\n",
    "popularity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
